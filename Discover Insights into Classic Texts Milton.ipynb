{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74ff79ad",
   "metadata": {},
   "source": [
    "# Discover Insights into Classic Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a0bb37",
   "metadata": {},
   "source": [
    "Novels and text contain insights into ideologies and places that are often originally unknown to the reader. By reading a written piece, you uncover the opinions of the author on their chosen topic and come to understand both the topic and how the author thinks.\n",
    "\n",
    "In this project we will perform a natural language parsing analysis to gain deeper insight into one of two famous and often discussed novels in the public domain: <a href=\"http://www.gutenberg.org/ebooks/174\" target=\"_blank\" rel=\"noopener noreferrer\">Oscar Wilde's _The Picture of Dorian Gray_</a> or <a href=\"http://www.gutenberg.org/ebooks/6130\" target=\"_blank\" rel=\"noopener noreferrer\"> Homer's _The Iliad!_</a> \n",
    "\n",
    "By the end of this project, we will find out the main topics of discussion in the novel of choosing and can begin to discern some of the author's thoughts and beliefs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf3caef",
   "metadata": {},
   "source": [
    "## Import and Preprocess Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359007fa",
   "metadata": {},
   "source": [
    "1. Import the text of choosing, convert it to lowercase, and name it. Also import other useful functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e98c5907",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/miltonmc5/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/miltonmc5/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer, word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def word_sentence_tokenize(text):\n",
    "  \n",
    "  # create a PunktSentenceTokenizer\n",
    "  sentence_tokenizer = PunktSentenceTokenizer(text)\n",
    "  \n",
    "  # sentence tokenize text\n",
    "  sentence_tokenized = sentence_tokenizer.tokenize(text)\n",
    "  \n",
    "  # create a list to hold word tokenized sentences\n",
    "  word_tokenized = list()\n",
    "  \n",
    "  # for-loop through each tokenized sentence in sentence_tokenized\n",
    "  for tokenized_sentence in sentence_tokenized:\n",
    "    # word tokenize each sentence and append to word_tokenized\n",
    "    word_tokenized.append(word_tokenize(tokenized_sentence))\n",
    "    \n",
    "  return word_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7af0c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# function that pulls chunks out of chunked sentence and finds the most common chunks\n",
    "def np_chunk_counter(chunked_sentences):\n",
    "\n",
    "    # create a list to hold chunks\n",
    "    chunks = list()\n",
    "\n",
    "    # for-loop through each chunked sentence to extract noun phrase chunks\n",
    "    for chunked_sentence in chunked_sentences:\n",
    "        for subtree in chunked_sentence.subtrees(filter=lambda t: t.label() == 'NP'):\n",
    "            chunks.append(tuple(subtree))\n",
    "\n",
    "    # create a Counter object\n",
    "    chunk_counter = Counter()\n",
    "\n",
    "    # for-loop through the list of chunks\n",
    "    for chunk in chunks:\n",
    "        # increase counter of specific chunk by 1\n",
    "        chunk_counter[chunk] += 1\n",
    "\n",
    "    # return 30 most frequent chunks\n",
    "    return chunk_counter.most_common(30)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# function that pulls chunks out of chunked sentence and finds the most common chunks\n",
    "def vp_chunk_counter(chunked_sentences):\n",
    "\n",
    "    # create a list to hold chunks\n",
    "    chunks = list()\n",
    "\n",
    "    # for-loop through each chunked sentence to extract verb phrase chunks\n",
    "    for chunked_sentence in chunked_sentences:\n",
    "        for subtree in chunked_sentence.subtrees(filter=lambda t: t.label() == 'VP'):\n",
    "            chunks.append(tuple(subtree))\n",
    "\n",
    "    # create a Counter object\n",
    "    chunk_counter = Counter()\n",
    "\n",
    "    # for-loop through the list of chunks\n",
    "    for chunk in chunks:\n",
    "        # increase counter of specific chunk by 1\n",
    "        chunk_counter[chunk] += 1\n",
    "\n",
    "    # return 30 most frequent chunks\n",
    "    return chunk_counter.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b207551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag, RegexpParser\n",
    "\n",
    "# import text of choice here\n",
    "the_iliad = open(\"the_iliad.txt\",encoding='utf-8').read().lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0010d6bd",
   "metadata": {},
   "source": [
    "2. With the text imported, we need to split the text into individual sentences and then individual words. This allows us to perform a sentence-by-sentence parsing analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc47040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence and word tokenize text here\n",
    "word_tokenized_text = word_sentence_tokenize(the_iliad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5ce7fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['consistency', 'is', 'no', 'less', 'pertinacious', 'and', 'exacting', 'in', 'its', 'demands', '.']\n"
     ]
    }
   ],
   "source": [
    "# store and print any word tokenized sentence here\n",
    "single_word_tokenized_sentence = word_tokenized_text[90]\n",
    "print(single_word_tokenized_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191d1eb1",
   "metadata": {},
   "source": [
    "## Part-of-speech Tag Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9e5d1f",
   "metadata": {},
   "source": [
    "4. Next we can part-of-speech tag each sentence to allow for syntax parsing. The list named `pos_tagged_text` will hold each part-of-speech tagged sentence from the novel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0603deee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagged_text = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e43a24a",
   "metadata": {},
   "source": [
    "5. Loop through each word tokenized sentence in `word_tokenized_text` and part-of-speech tag each sentence using `nltk`'s `pos_tag()` function. Append the result to `pos_tagged_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b39a4538",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ws in word_tokenized_text:\n",
    "    pos_tagged_text.append(pos_tag(ws))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8205d88f",
   "metadata": {},
   "source": [
    "6. Save any part-of-speech tagged sentence in `pos_tagged_text` to a variable named `single_pos_sentence`. Print `single_pos_sentence` as a check to visualize what you have done so far!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8af95d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('consistency', 'NN'), ('is', 'VBZ'), ('no', 'DT'), ('less', 'RBR'), ('pertinacious', 'JJ'), ('and', 'CC'), ('exacting', 'VBG'), ('in', 'IN'), ('its', 'PRP$'), ('demands', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# store and print any part-of-speech tagged sentence here\n",
    "single_pos_sentence = pos_tagged_text[90]\n",
    "print(single_pos_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be552549",
   "metadata": {},
   "source": [
    "## Chunk Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50c8554",
   "metadata": {},
   "source": [
    "7. Now that we have part-of-speech tagged our text, we can move on to syntax parsing.\n",
    "\n",
    "   Begin by defining a piece of chunk grammar `np_chunk_grammar` that will chunk a noun phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4333ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a794295",
   "metadata": {},
   "source": [
    "8. Create a `nltk` `RegexpParser` object named `np_chunk_parser` using the noun phrase chunk grammar we defined as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c985dcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_chunk_parser = RegexpParser(np_chunk_grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d08787c",
   "metadata": {},
   "source": [
    "9. Define a piece of chunk grammar named `vp_chunk_grammar` that will chunk a verb phrase of the following form: noun phrase, followed by a verb `VB`. followed by an optional adverb `RB`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f807a549",
   "metadata": {},
   "outputs": [],
   "source": [
    "vp_chunk_grammar = \"VP: {<DT>?<JJ>*<NN><VB.*><RB.?>?}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d553fbb0",
   "metadata": {},
   "source": [
    "10. Create a `nltk` `RegexpParser` object named `vp_chunk_parser` using the verb phrase chunk grammar we defined as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ee7f3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vp_chunk_parser = RegexpParser(vp_chunk_grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51341fa",
   "metadata": {},
   "source": [
    "11. Create two empty lists `np_chunked_text` and `vp_chunked_text` that will hold the chunked sentences from our text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "047c8383",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_chunked_text = []\n",
    "vp_chunked_text = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cf6425",
   "metadata": {},
   "source": [
    "12. Loop through each part-of-speech tagged sentence in `pos_tagged_text` and noun phrase chunk each sentence using our `RegexpParser`'s `.parse()` method. Append the result to `np_chunked_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "162d0e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ws in pos_tagged_text:\n",
    "    np_chunked_text.append(np_chunk_parser.parse(ws))\n",
    "    vp_chunked_text.append(vp_chunk_parser.parse(ws))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cab095",
   "metadata": {},
   "source": [
    "13. Within the same loop we defined in the previous task, verb phrase chunk each part-of-speech tagged sentence using your `RegexpParser`'s `.parse()` method. Append the result to `vp_chunked_text`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ade8062",
   "metadata": {},
   "source": [
    "## Analyze Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4213383",
   "metadata": {},
   "source": [
    "14. Now that we have chunked our novel, we can analyze the chunk frequencies to gain insights.\n",
    "\n",
    "    A function `np_chunk_counter()` that returns the `30` most common NP-chunks from a list of chunked sentences is defined at the beginning of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "17916ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((('hector', 'NN'),), 322), ((('i', 'NN'),), 277), ((('jove', 'NN'),), 257), ((('troy', 'NN'),), 208), ((('vain', 'NN'),), 195), ((('war', 'NN'),), 193), ((('son', 'NN'),), 170), ((('thou', 'NN'),), 158), ((('the', 'DT'), ('plain', 'NN')), 157), ((('the', 'DT'), ('field', 'NN')), 154), ((('the', 'DT'), ('ground', 'NN')), 138), ((('death', 'NN'),), 134), ((('hand', 'NN'),), 134), ((('greece', 'NN'),), 128), ((('heaven', 'NN'),), 127), ((('fate', 'NN'),), 127), ((('thee', 'NN'),), 122), ((('breast', 'NN'),), 121), ((('the', 'DT'), ('trojan', 'NN')), 120), ((('the', 'DT'), ('god', 'NN')), 119), ((('the', 'DT'), ('war', 'NN')), 117), ((('the', 'DT'), ('greeks', 'NN')), 116), ((('blood', 'NN'),), 115), ((('homer', 'NN'),), 112), ((('the', 'DT'), ('king', 'NN')), 105), ((('rage', 'NN'),), 103), ((('force', 'NN'),), 103), ((('care', 'NN'),), 99), ((('head', 'NN'),), 98), ((('man', 'NN'),), 97)]\n"
     ]
    }
   ],
   "source": [
    "most_common_np_chunks = np_chunk_counter(np_chunked_text)\n",
    "print(most_common_np_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db89a4b7",
   "metadata": {},
   "source": [
    "15. A function `vp_chunk_counter()` that returns the `30` most common VP-chunks from a list of chunked sentences is also defined at the beginning of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9cfdc4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(((\"'t\", 'NN'), ('is', 'VBZ')), 19), ((('i', 'NN'), ('am', 'VBP')), 11), (((\"'t\", 'NN'), ('was', 'VBD')), 11), ((('the', 'DT'), ('hero', 'NN'), ('said', 'VBD')), 9), ((('i', 'NN'), ('know', 'VBP')), 8), ((('i', 'NN'), ('saw', 'VBD')), 8), ((('the', 'DT'), ('scene', 'NN'), ('lies', 'VBZ')), 7), ((('i', 'NN'), ('was', 'VBD')), 6), ((('confess', 'NN'), (\"'d\", 'VBD')), 6), ((('the', 'DT'), ('scene', 'NN'), ('is', 'VBZ')), 6), ((('view', 'NN'), (\"'d\", 'VBD')), 5), ((('i', 'NN'), ('felt', 'VBD')), 5), ((('i', 'NN'), ('bear', 'VBP')), 5), ((('hector', 'NN'), ('is', 'VBZ')), 5), ((('vain', 'NN'), ('was', 'VBD')), 5), ((('homer', 'NN'), ('was', 'VBD')), 4), ((('i', 'NN'), ('have', 'VBP')), 4), ((('hunger', 'NN'), ('was', 'VBD')), 4), ((('glory', 'NN'), ('lost', 'VBN')), 4), ((('i', 'NN'), ('see', 'VBP')), 4), ((('war', 'NN'), ('be', 'VB')), 4), ((('the', 'DT'), ('weapon', 'NN'), ('stood', 'VBD')), 4), ((('i', 'NN'), ('go', 'VBP')), 4), ((('the', 'DT'), ('silence', 'NN'), ('broke', 'VBD')), 4), ((('the', 'DT'), ('trojan', 'NN'), ('bands', 'VBZ')), 4), ((('father', 'NN'), ('gave', 'VBD')), 4), ((('i', 'NN'), ('deem', 'VBP')), 4), ((('minerva', 'NN'), ('repressing', 'VBG')), 3), ((('thetis', 'NN'), ('calling', 'VBG')), 3), ((('thetis', 'NN'), ('entreating', 'VBG')), 3)]\n"
     ]
    }
   ],
   "source": [
    "most_common_vp_chunks = vp_chunk_counter(vp_chunked_text)\n",
    "print(most_common_vp_chunks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
